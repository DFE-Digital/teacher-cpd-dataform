## Pipeline Monitoring

### Slack Channel: #cpd-dataform-monitoring 

### Pipeline failures 

The teacher-cpd-dfe-analytics-dataform pipeline pulls out event data that is streamed to BigQuery by dfe-analytics and transforms it into tables.
These tables then act as source tables for the teacher-cpd-dataform pipeline, which transforms them further into 'mart' tables.
The pipelines are run each weekday at 5am and each hour from 8am to 5pm; the first pipeline runs on the hour and the second 10 minutes later.

Alerts are set up through Google Cloud’s Monitoring service.
Four “policies” check if the Dataform pipeline has run, one for a successful run and one for a failed run, for each of the 2 pipelines.
In either case, an email is sent to the Slack channel #cpd-dataform-monitoring informing us of the status of the pipeline. 

If there is a failure, the cause will be outlined in the Workflow Execution Logs in Dataform.
Many problems are indicated by a failed "assertion" and can be further investigated by querying the view that represents the failed assertion in that workflow. 

### Assertions and fixing pipeline problems 

Assertions are data quality test queries that find rows in a table that violate one or more rules specified in the query.
Assertions provided by the dfe-analytics-dataform Dataform package help spot when your dfe_analytics_dataform configuration JSON parameter has become out of date or has a problem.
Fixing them often requires part(s) of the dfe_analytics_dataform configuration JSON parameter to be updated.
This is located in a .js file in the definitions/ folder in the teacher-cpd-dfe-analytics-dataform Dataform repository, and may be updated in a development workspace there.
There are 3 of these .js files, one for each application that data is streamed from: ecf1, ecf2 & npq. Failed assertion names contain these short strings to indicate which file/pipeline the issue originates from.
These strings are expressed as XXX in the examples below.

The most common assertion failures are: 

- XXX_entities_are_missing_expected_fields – data has been received about an entity in the database that is missing a field it was expecting. This failure will generate an error and prevent further queries in your pipeline from running. 
- XXX_unhandled_field_or_entity_is_being_streamed – data has been received about an entity in the database that contains a field (or entire table) it was not expecting. This failure will generate an error but will not prevent further queries in your pipeline from running. However, the new field(s) or table will not be included in dfe-analytics-dataform output until you update your configuration, and the error will continue to reoccur. 

In either case, you should update your dataSchema configuration in dfe_analytics_dataform.js in Dataform to add or remove configuration for that field. 

The following assertion failures are also possible:

- XXX_hidden_pii_configuration_does_not_match_entity_events_streamed_yesterday - there is a mismatch in some of yesterday’s streamed events between the fields configured to be hidden by dfe-analytics in a config/analytics_hidden_pii.yml file in the application GitHub repo and the dfe-analytics-dataform dataSchema input parameter. This could be because a field has been hidden in dfe-analytics but not in dfe-analytics-dataform, or a field has been unhidden in dfe-analytics but not in dfe-analytics-dataform. To fix this, (1) open the failed assertion in Dataform (generated by dfe_analytics_dataform_….js) and re-run it (2) look at the results to deduce the cause of the issue (3) update the value of the ‘hidden’ parameter for affected fields to true or false as appropriate and merge (4) re-run the whole pipeline (a Github action should trigger this automatically) (5) Run the migrate_historic_XXX_events_to_current_hidden_pii_configuration stored procedure in the dataform dataset to migrate all data in the events table to the new hidden PII configuration. FYI: Once you have fixed the issue at source, and then run the “migrate historic events to current hidden pii configuration” stored procedure the events from yesterday will not be hidden, because the procedure will not amend the historic PII for the previous day. This alert will continue to fire off until 24 hours has passed since the last hidden event. This means, that if this alert fires off, it will be a full 2 days until you can run the pipeline successfully again.  
- XXX_hidden_pii_configuration_does_not_matchsample_of_historic_entity_events_streamed_yesterday - there is a mismatch in a sample of historic streamed events between the fields configured to be hidden by dfe-analytics in the config/analytics_hidden_pii.yml file in the application repo and the dfe-analytics-dataform dataSchema input parameter. This could be because hidden field configuration has changed but the migrate_historic_..._events_to_current_hidden_pii_configuration stored procedure has not been run to migrate historic events to the new configuration, or something happened that would ordinarily trigger XXX_hidden_pii_configuration_does_not_match_entity_events_streamed_yesterday to fail, but which didn’t because the affected table(s) weren’t updated yesterday. To fix this, (1) Open the failed assertion in Dataform (generated by XXX_dfe_analytics_dataform.js) and re-run it (2) Cross reference the results with the analytics_hidden_pii.yml file to deduce the cause of the issue (see left) (3) If something happened that would ordinarily trigger XXX_hidden_pii_configuration_does_not_match_entity_events_streamed_yesterday  to fail, but which didn’t because the affected table(s) weren’t updated yesterday follow the steps for that assertion (4) If hidden field configuration has changed but the migrate_historic_XXX_events_to_current_hidden_pii_configuration stored procedure has not been run to migrate historic events to the new configuration, run that stored procedure. 
- XXX_entities_have_not_been_backfilled - Data for a particular table is streaming from dfe-analytics but no backfill (set of import_entity events) is present in the events table. This means that the contents of this table will probably be incomplete. This could be because the table is new but no backfill has ever been run for it, the entire service is new and no whole-database backfill has been run as part of initial onboarding or all backfills for the table have been deleted e.g. as part of enforcement of a data retention schedule. To fix this, (1) open the failed assertion in Dataform (generated by dfe_analytics_dataform_….js) and re-run it to identify which table(s) are missing backfills and then (2) ask developers for that application to run a backfill for either the affected tables or the whole database as appropriate: https://github.com/DFE-Digital/dfe-analytics?tab=readme-ov-file#7-import-existing-data 
- XXX_data_not_fresh_... - either no new entity CRUD event data has been received for the entity and application listed in the error in the last X days (where X is the value of the dataFreshnessDays parameter configured in the dataSchema for that entity/application). The root cause is most likely just a quiet period in which no changes were made to the source database table for that entity, but could be more serious if not, OR no new custom events have been received in the last X days for a particular custom event. To fix this, If a one off quiet period - do nothing. If this is a repeated quiet period (e.g. weekends, or an entity that is only rarely updated) consider increasing dataFreshnessDays for that entity/custom event or removing it entirely. Alternatively if it seems odd that no changes at all would be made to that entity in the last X days, ask the relevant team to check whether this is the case - if updates were made then this should be passed on to a dfe-analytics developer for investigation. 

If an assertion failure is not listed here or you require further support please consult the Schools Digital Data & Insights team at #sd_data_insights.